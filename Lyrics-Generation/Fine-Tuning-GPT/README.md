# Fine-Tuning GPT-2 Model

## Overview

**Fine-Tuning GPT-2** is part of the **Lyrics Generation**, designed to load, process, and fine-tune a specialized Spanish language model. This project uses the pre-trained [datificate/gpt2-small-spanish](https://huggingface.co/datificate/gpt2-small-spanish) model from **Hugging Face** to tackle Spanish text generation tasks. The workflow begins with data gathered in the **Data Extraction** phase, which is then divided into four parts to evaluate the model's performance at different stages of training.


**Content:**
- [Architecture](#architecture)
- [Generative Pretrained Transformer (GPT)](#generative-pretrained-transformer-gpt)
    - [Key Components of GPT Architecture](#key-components-of-gpt-architecture)
    - [Diagram: GPT Architecture](#diagram-gpt-architecture)
- [Model: `datificate/gpt2-small-spanish`](#model-datificategpt2-small-spanish)
    - [Key Features of the Model](#key-features-of-the-model)
    - [Reasons for Choosing `datificate/gpt2-small-spanish`](#reasons-for-choosing-datificategpt2-small-spanish)
- [Data Preparation and Splitting](#data-preparation-and-splitting)
- [Fine Tuning Process](#fine-tuning-process)
- [Evaluation Metrics](#evaluation-metrics)
- [Results and Analysis](#results-and-analysis)
   - [Log Loss Analysis for Fine-Tuning](#log-loss-analysis-for-fine-tuning)
   - [Metrics Analysis](#metrics-analysis)
- [Prerequisites](#prerequisites)

---

## Architecture

<p align="center">
  <img src="https://github.com/user-attachments/assets/c3725dc4-d1e9-4cd3-aea5-0a7b47e81ea1" alt="Architecture">
</p>

<p align="center">
  <em>Figure 1: Architecture of Fine Tuning GPT-2</em>
</p>


---

## Generative Pretrained Transformer (GPT)

GPT is a language model architecture that uses the *Transformers* approach. It was originally introduced in 2017 by Vaswani et al. and is designed to learn complex language representations and generate text autoregressively.


### Key Components of GPT Architecture

1. **Embedding Layer**:
   - Converts words or tokens into fixed-dimension numerical vectors that semantically represent the input texts.

2. **Transformer Blocks**:
   - **Self-Attention Mechanism**:
     Allows the model to identify and prioritize key relationships between words within a sequence.
   - **Feedforward Layers**:
     Fully connected neural networks that process the representations generated by attention to extract higher-level features.

3. **Attention Mask**:
   - In GPT, the attention is *causal*, ensuring the model only considers previous words to predict the next word in the sequence.

4. **Decoding Layer**:
   - Produces the probability of each possible word based on the given context.


### Diagram: GPT Architecture

<p align="center">
  <img src="https://github.com/user-attachments/assets/25ccedca-5493-4854-b859-563c6c37af61" alt="Architecture">
</p>

<p align="center">
  <em>Figure 2: Architecture of GPT</em>
</p>


---

## Model: `datificate/gpt2-small-spanish`

The [datificate/gpt2-small-spanish](https://huggingface.co/datificate/gpt2-small-spanish) model is an adjusted version of GPT-2, optimized for tasks in the Spanish language. This pretrained model is based on the Transformer architecture, designed to understand and generate fluent and contextual text in Spanish, making it ideal for text generation projects.

### Key Features of the Model

1. **Based on GPT-2**:
   - An autoregressive model designed to predict the next word in a sequence using the previous context.
   - Optimized to maintain coherence, fluency, and grammatical correctness in generated texts.

2. **Specialized Pretraining**:
   - Trained on a massive corpus of Spanish texts, ensuring a strong grasp of the language’s lexical, grammatical, and semantic structures.

3. **Compact Model ("small")**:
   - A configuration with approximately 124 million parameters, balancing performance and computational efficiency.

4. **Extensive Vocabulary**:
   - Utilizes a tokenizer with a vocabulary size of 50,257 tokens, enabling the model to handle a wide range of words and expressions in Spanish.

5. **Application Possibilities**:
   - Automated text generation (stories, song lyrics, poetry).
   - Text completion or suggestion tasks in Spanish.
   - Integration into virtual assistants or chatbots.



### Reasons for Choosing `datificate/gpt2-small-spanish`

- **Specialization in Spanish:**
    - This model was pretrained exclusively on Spanish-language data, ensuring a deep understanding of the language's specific nuances.

-  **Computational Efficiency:**
    - Its reduced size (124M parameters) allows for quick training, requiring fewer computational resources and less time.

- **Flexibility:**
    - The compact architecture makes it easier to adapt the model for custom tasks, such as lyric generation or virtual assistant creation.

- **Availability and Community:**
    - Hosted on Hugging Face, it comes with detailed documentation, practical examples, and an active community that simplifies its implementation.


---

## Data Preparation and Splitting

To effectively fine-tune the `datificate/gpt2-small-spanish` model, it's essential to prepare and structure the data appropriately. Here's an overview of the process:

1. **Data Splitting**:
   - The `data_split.py` script is designed to divide the collected dataset into four distinct parts. 
   - This segmentation facilitates the fine-tuning process by allowing the model to train on varied subsets, enhancing its adaptability and performance.

2. **Model and Tokenizer Loading**:
   - Before processing the data, it's crucial to load the pre-trained model and its associated tokenizer.
   - The tokenizer converts raw text into a format the model can understand, ensuring seamless integration between data and model.

3. **Dataset Loading**:
   - After splitting the data, each segment is loaded as a dataset. This step organizes the data into a structured format, making it ready for the fine-tuning process.

4. **Tokenization**:
   - The tokenizer processes the text data, preparing it for the model. This involves converting text into tokens and ensuring each sequence is of appropriate length, which is vital for efficient model training.


--- 

## Fine Tuning Process

To fine-tune the `datificate/gpt2-small-spanish` model for lyric generation, the following steps were undertaken:

1. **Environment Configuration**:
   - Disabled the Weights & Biases integration to streamline the training process.

2. **Training Parameters Setup**:
   - Specified the directory to save trained models.
   - Assigned a name to the training run for identification.
   - Enabled overwriting of previous outputs to ensure the latest training results are saved.
   - Set the number of training epochs to 3, determining how many times the model will process the entire dataset.
   - Defined a batch size of 4, indicating the number of samples the model processes before updating its parameters.
   - Configured the model to save its state every 500 steps, allowing for checkpoints during training.
   - Limited the number of saved checkpoints to the two most recent ones to manage storage efficiently.
   - Designated a directory for logging training information.
   - Set logging to occur every 1,000 steps to monitor training progress.
   - Enabled the training process while skipping evaluation during this phase.
   - Included a warm-up phase of 100 steps to help the model adjust gradually to the learning process.
   - Established a learning rate of 5e-5, controlling the speed at which the model updates its knowledge.
   - Applied a weight decay of 0.01 to prevent overfitting by slightly penalizing large weights.

3. **Trainer Initialization**:
   - Created a trainer object by combining the model, the defined training parameters, and the prepared dataset.

4. **Training Execution**:
   - Initiated the training process, allowing the model to learn from the dataset based on the specified configurations.


---

## Evaluation Metrics

These four metrics are used to evaluate text generated by language models, explaining their purpose, mathematical foundation, and what they measure.


### Perplexity

- Perplexity evaluates how well a language model predicts a sequence of text. It indicates the "surprise" of the model when seeing the generated text.
- It is calculated as the inverse of the average probability of the generated text, normalized by the text length:

$$
\text{Perplexity} = e^{-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i)}
$$

- $N$ is the total number of words and $P(w_i)$ is the probability assigned by the model to the word $w_i$.

- Lower perplexity suggests that the model generates plausible and grammatically correct text based on its training.


### Distinct-n

- Measures the diversity of the generated text by evaluating the proportion of unique n-grams to the total n-grams generated.

- For a text $T$ with $n$-grams $\{g_1, g_2, \ldots, g_N\}$:

$$
\text{Distinct-n} = \frac{\text{Number of unique n-grams}}{\text{Total number of n-grams}}
$$

- Evaluates the model's creativity and avoids excessive repetitions in the generated text.


### Lexical Diversity

- Measures the proportion of unique words relative to the total number of words in the generated text.
- If the text contains $W$ words and $U$ of them are unique:

$$
\text{Lexical Diversity} = \frac{|U|}{|W|}
$$

- Indicates the richness of vocabulary used in the text. A low value suggests repetition, while a high value indicates variety.


### Local Coherence

- Evaluates how well consecutive sentences are semantically related.
- For a text $T$ divided into sentences $S_1, S_2, \ldots, S_n$, local coherence is calculated as the average cosine similarity between consecutive sentences:

$$
\text{Local Coherence} = \frac{1}{n-1} \sum_{i=1}^{n-1} \cos(\text{emb}(S_i), \text{emb}(S_{i+1}))
$$

- $\text{emb}(S)$ is the semantic embedding of the sentence $S$.

- Ensures that the text has logical flow, with sentences meaningfully connected.


### Summary

| Metric                | What it Measures                              | Optimal Value                      |
|-----------------------|-----------------------------------------------|------------------------------------|
| **Perplexity**        | Plausibility of the text according to the model | Lower is better (e.g., <30 for coherent text) |
| **Distinct-n**        | Diversity of unique n-grams                   | Higher is better (e.g., >0.8 for creative text) |
| **Lexical Diversity** | Vocabulary richness                           | Higher is better (e.g., >0.5 for varied text) |
| **Local Coherence**   | Semantic relation between consecutive sentences | Higher is better (e.g., >0.7 for well-connected sentences) |


--- 

## Results and Analysis

### Log Loss Analysis for Fine-Tuning

The *fine-tuning* process for the `GPT-2` model was conducted in four stages, using different subsets of the dataset. Each iteration optimized the model's ability to generate coherent and relevant text. During training, the *Training Loss* was recorded as a function of the *Steps*.

The following graph illustrates the evolution of *Training Loss* for each stage of *fine-tuning*:


<p align="center">
  <img src="https://github.com/user-attachments/assets/9591923c-8259-46e9-8a7a-c915fab25fd2" alt="Architecture">
</p>

<p align="center">
  <em>Figure 2: Training Loss by Step for each Fine-Tuning</em>
</p>


**Observations**

- The *fine-tuning* process achieved a consistent decrease in *Training Loss*, indicating that the model progressively adapted to the data.
- The stages `fine-tuning_1` and `fine-tuning_3` exhibit lower final loss, suggesting that the subsets used in these phases were more suitable.
- The high initial loss in `fine-tuning_1` reflects greater complexity or noise in the initial data. However, the model successfully adapted over time, learning the underlying patterns in the dataset.


### Metrics Analysis

#### Perplexity

- Perplexity decreases significantly in `Fine Tuning 3` and `Fine Tuning 4`, indicating a better model fit after more fine-tuning iterations.
- `Fine Tuning 2` shows an increase in perplexity to 87.86, which could indicate overfitting or a mismatch in the training process.
- In `Fine Tuning 3`, perplexity drops sharply to 42.92, demonstrating a substantial improvement in model adaptation.
- `Fine Tuning 4` achieves the lowest perplexity of 36.79, highlighting the positive impact of the final iterations on the model's performance.

<p align="center">
  <img src="https://github.com/user-attachments/assets/442a38f5-1762-447c-bdec-c83ca1d3be89" alt="Architecture">
</p>

<p align="center">
  <em>Figure 3: Perplexity by Step for each Fine-Tuning</em>
</p>



#### Distinct-2

- The `Distinct-2` metric remains high in `Fine Tuning 1`, `Fine Tuning 3`, and `Fine Tuning 4`, indicating a consistent ability to generate diverse outputs across these phases.
- A slight decrease in `Distinct-2` is observed in `Fine Tuning 2`, dropping to 0.98, which may reflect a temporary reduction in diversity during this phase.
- In `Fine Tuning 3`, the `Distinct-2` value recovers to 0.99, demonstrating an improvement in output diversity after adjustments.
- `Fine Tuning 4` sustains a high `Distinct-2` value of 0.99, consolidating the model's ability to generate diverse responses in the final phase.

<p align="center">
  <img src="https://github.com/user-attachments/assets/39709813-21b7-4ab3-81a7-a536e93661ff" alt="Architecture">
</p>

<p align="center">
  <em>Figure 4: Distinct-2 by Step for each Fine-Tuning</em>
</p>



#### Lexical Diversity


- The `Lexical Diversity` metric starts high at 0.99 in `Fine Tuning 1`, indicating a strong variety in generated outputs initially.
- A progressive decrease is observed, with `Fine Tuning 2` dropping to 0.96 and reaching the lowest point of 0.92 in `Fine Tuning 3`, suggesting a reduction in diversity during these phases.
- In `Fine Tuning 4`, the `Lexical Diversity` value recovers significantly to 0.99, demonstrating a restoration of the model's ability to produce varied outputs.
- The overall trend indicates a temporary decline in diversity during intermediate phases, followed by a strong recovery in the final fine-tuning iteration.

<p align="center">
  <img src="https://github.com/user-attachments/assets/6cf0aefa-5bf5-440d-a3d3-2d3b1a97df2e" alt="Architecture">
</p>

<p align="center">
  <em>Figure 5: Lexical Diversity by Step for each Fine-Tuning</em>
</p>



#### Local Coherence

- `Local Coherence` starts relatively low at 0.37 in `Fine Tuning 1`, indicating initial challenges in maintaining coherent outputs.
- In `Fine Tuning 2`, there is a significant improvement, with `Local Coherence` peaking at 0.49, suggesting better contextual consistency.
- A sharp decline occurs in `Fine Tuning 3`, where `Local Coherence` drops to 0.39, reflecting a potential reduction in the model's ability to maintain coherence.
- `Fine Tuning 4` shows recovery with an increase to 0.45, demonstrating partial restoration of coherence in the final phase of fine-tuning.

<p align="center">
  <img src="https://github.com/user-attachments/assets/3701a589-b93f-451f-a91c-17c41edc284c" alt="Architecture">
</p>

<p align="center">
  <em>Figure 6: Local Coherence by Step for each Fine-Tuning</em>
</p>



---

## Prerequisites

To install the necessary packages for `Training`, execute the following command:

```bash
pip install transformers datasets
```

To install the necessary packages for `Evaluation`, execute the following command:

```bash
pip install transformers sentence-transformers nltk torch
```



## References


1. Obregón, J. and Carrera, B. (2020).  
  **"Gpt2-small-spanish: Un modelo de lenguaje para generación de texto en español".**  
  URL: [https://huggingface.co/daticfate/gpt2-small-spanish](https://huggingface.co/daticfate/gpt2-small-spanish)

2. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017).  
  **"Attention is all you need".**  
  En *Advances in Neural Information Processing Systems*, Vol. 30.

3. Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer.  
  **"A Statistical Approach to Machine Translation."**  
  *Computational Linguistics*, 16(2):79-85, 1990.